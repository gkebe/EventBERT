Convert XLNet model to huggingface
    export TRANSFO_XL_CHECKPOINT_PATH=~/Nvidia-Bert/model/xlnet/200000/model.ckpt
    export TRANSFO_XL_CONFIG_PATH=~/Nvidia-Bert/xlnet_config.json
    export PYTORCH_DUMP_OUTPUT=~/Nvidia-Bert/model/xlnet/200000

    transformers-cli convert --model_type xlnet --tf_checkpoint $TRANSFO_XL_CHECKPOINT_PATH --config $TRANSFO_XL_CONFIG_PATH --pytorch_dump_output $PYTORCH_DUMP_OUTPUT

BERT data preprocessing:
    wiki_70k:
        python eventPrep.py --input_file wiki_70k/step1/train.txt --output_file wiki_70k/step2/train.txt
        with labels:
            python eventPrep.py --input_file wiki_70k/step1/train.txt --output_file wiki_70k/step2/train.txt --keep_label True
        tuples as sentences:
            python eventPrep.py --input_file wiki_70k/step1/train.txt --output_file wiki_70k/step2/train.txt --tuple_to_sen True\
        awk '{filename = "wiki_70k/step3/wiki_70k_training_" int((NR-1)/80000) ".txt"; print >> filename}' wiki_70k/step2/train.txt
        python eventHdf5.py --dataset wiki_70k --max_seq_length 128 --max_predictions_per_seq 20 --vocab_file download/google_pretrained_weights/uncased_L-12_H-768_A-12/vocab.txt --do_lower_case 1 --n_training_shards 210 --n_test_shards 0
    PRES:
        python eventPrep.py --input_file="PRES/step1/train.txt" --data_type PRES --output_file PRES/step2/train.txt
        awk '{filename = "PRES/step3/PRES_training_" int((NR-1)/80000) ".txt"; print >> filename}' PRES/step2/train.txt
        python eventHdf5.py --dataset PRES --max_seq_length 128 --max_predictions_per_seq 20 --vocab_file download/google_pretrained_weights/uncased_L-12_H-768_A-12/vocab.txt --do_lower_case 1 --n_training_shards 13 --n_test_shards 0
        from step2:
            python eventHdf5.py --dataset PRES --input_dir PRES/step2/ --max_seq_length 128 --max_predictions_per_seq 20 --vocab_file download/google_pretrained_weights/uncased_L-12_H-768_A-12/vocab.txt --do_lower_case 1 --n_training_shards 1 --n_test_shards 0

Pretraining BERT model:
    bash scripts/run_pretraining.sh -n 1 -g "2" -c checkpoints/bert-base.pt -a 81920 -d PRES -z 0 -w 794

GPT-2 data preprocessing:
    python data/eventPrep.py --input_file data/wiki_70k/step1/train.txt --output_file data/wiki_70k/step2/train_gpt2.txt  --add_tup True
    python encode.py ../Nvidia-Bert/data/wiki_70k/step2/train_gpt2.txt wiki_70k.npz

Pretraining GPT-2 model
    python train.py --dataset="wiki_70k.npz" --batch_size=10 --run_name run2

Convert GPT-2 model to huggingface
    export OPENAI_GPT2_CHECKPOINT_PATH=~/Nvidia-Bert/model/gpt2/348000
    export PYTORCH_DUMP_OUTPUT=~/Nvidia-Bert/model/gpt2/348000

    transformers-cli convert --model_type gpt2 --tf_checkpoint OPENAI_GPT2_CHECKPOINT_PATH --pytorch_dump_output $PYTORCH_DUMP_OUTPUT

Generate events with BERT
    bash scripts/run_generation.sh -c model/ckpt_794.pt -g "3" -s [CLS],build

Generate events with XLNet
    python run_generation.py --model_type xlnet --model_name xlnet-base-cased --model_name_or_path model/xlnet/pytorch_model.bin --num_return_sequences 10 --no_tokenizer

Generate events with GPT-2
    python run_generation.py --model_type gpt2 --model_name gpt2 --model_name_or_path model/gpt2/348000/ --num_return_sequences 10

Inverse Cloze with Bert
    bash scripts/run_inverse_cloze.sh -c checkpoints/bert-base.pt -i cloze_dataset_weber_test.json -g "2"

Inverse Cloze with XLNet
    bash scripts/run_inverse_cloze_xlnet.sh -c model/xlnet/200000/pytorch_model.bin -i cloze_dataset_weber.json -g "2"

Inverse Cloze with GPT-2
    bash scripts/run_inverse_cloze_gpt2.sh -c model/gpt2/200000/pytorch_model.bin -i cloze_dataset_weber.json -g "2"