# -*- coding: utf-8 -*-
"""frames.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QrsgTcHqjR_RkinYyFQVe7t3Tgo4IieM
"""
import string
from collections import Counter
from itertools import chain 
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from pytorch_transformers import BertTokenizer, BertForSequenceClassification, BertConfig, PretrainedConfig
from pytorch_transformers import AdamW
from tqdm import trange
import numpy as np
from sklearn.metrics import matthews_corrcoef, f1_score, recall_score, precision_score, confusion_matrix, classification_report
import csv
import sys
import os
import logging

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
n_gpu = torch.cuda.device_count()
torch.cuda.get_device_name(0)

logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt = '%m/%d/%Y %H:%M:%S',
                    level = logging.INFO)
logger = logging.getLogger(__name__)

def compute_metrics(task_name, preds, labels, label_names=None):
    if label_names is None:
        label_names = []
    assert len(preds) == len(labels)
    if task_name == "cola":
        return {"mcc": matthews_corrcoef(labels, preds)}
    elif task_name == "sst-2":
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "mrpc":
        return acc_and_f1(preds, labels)
    # elif task_name == "sts-b":
    #     return pearson_and_spearman(preds, labels)
    elif task_name == "qqp":
        return acc_and_f1(preds, labels)
    elif task_name == "mnli":
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "mnli-mm":
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "qnli":
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "rte":
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "wnli":
        return {"acc": simple_accuracy(preds, labels)}
    elif task_name == "frames":
        return metrics_frame(preds, labels,label_names)
    else:
        raise KeyError(task_name)


def simple_accuracy(preds, labels):
    return (preds == labels).mean()

def acc_and_f1(preds, labels):
    acc = simple_accuracy(preds, labels)
    f1 = f1_score(y_true=labels, y_pred=preds)
    return {
        "acc": acc,
        "f1": f1,
        "acc_and_f1": (acc + f1) / 2,
    }
def metrics_frame(preds, labels, label_names):
    recall_micro = recall_score(labels, preds, average="micro")
    recall_macro = recall_score(labels, preds, average="macro")
    precision_micro = precision_score(labels, preds, average="micro")
    precision_macro = precision_score(labels, preds, average="macro")
    f1_micro = f1_score(labels, preds, average="micro")
    f1_macro = f1_score(labels, preds, average="macro")
    cm = confusion_matrix(labels, preds)
    cr = classification_report(labels, preds, labels=list(range(len(label_names))), target_names=label_names)
    model_metrics = {"Precision, Micro": precision_micro, "Precision, Macro": precision_macro,
                     "Recall, Micro": recall_micro, "Recall, Macro": recall_macro,
                     "F1 score, Micro": f1_micro, "F1 score, Macro": f1_macro, "Confusion matrix": cm, "Classification report": cr}
    return model_metrics
class InputExample(object):
    """A single training/test example for simple sequence classification."""

    def __init__(self, guid, text_a, text_b=None, label=None):
        """Constructs a InputExample.

        Args:
            guid: Unique id for the example.
            text_a: string. The untokenized text of the first sequence. For single
            sequence tasks, only this sequence must be specified.
            text_b: (Optional) string. The untokenized text of the second sequence.
            Only must be specified for sequence pair tasks.
            label: (Optional) string. The label of the example. This should be
            specified for train and dev examples, but not for test examples.
        """
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.label = label


class InputFeatures(object):
    """A single set of features of data."""

    def __init__(self, input_ids, input_mask, segment_ids, label_id):
        self.input_ids = input_ids
        self.input_mask = input_mask
        self.segment_ids = segment_ids
        self.label_id = label_id


class DataProcessor(object):
    """Base class for data converters for sequence classification data sets."""

    def get_train_examples(self, data_dir):
        """Gets a collection of `InputExample`s for the train set."""
        raise NotImplementedError()

    def get_dev_examples(self, data_dir):
        """Gets a collection of `InputExample`s for the dev set."""
        raise NotImplementedError()

    def get_labels(self):
        """Gets the list of labels for this data set."""
        raise NotImplementedError()

    @classmethod
    def _read_tsv(cls, input_file, quotechar=None):
        """Reads a tab separated value file."""
        with open(input_file, "r", encoding="utf8") as f:
            reader = csv.reader(f, delimiter="\t", quotechar=quotechar)
            lines = []
            for line in reader:
                if sys.version_info[0] == 2:
                    line = list(cell.decode('utf-8') for cell in line)
                lines.append(line)
            return lines

class FramesProcessor(DataProcessor):
    """Processor for the Frames data set (Wiki_70k version)."""

    def get_train_examples(self, data_dir):
        """See base class."""
        logger.info("LOOKING AT {}".format(os.path.join(data_dir, "train.tsv")))
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "train.tsv")), "train")

    def get_dev_examples(self, data_dir):
        """See base class."""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "valid.tsv")), "dev")

    def get_test_examples(self, data_dir):
        """See base class."""
        return self._create_examples(
            self._read_tsv(os.path.join(data_dir, "test.tsv")), "test")

    def get_labels(self, data_dir):
        """See base class."""
        train_examples = self.get_train_examples(data_dir)
        dev_examples = self.get_dev_examples(data_dir)
        test_examples = self.get_test_examples(data_dir)
        return list(set([i.label for i in train_examples+dev_examples+test_examples]))

    def _create_examples(self, lines, set_type):
        """Creates examples for the training and dev sets."""
        examples = []
        for (i, line) in enumerate(lines):
            if i == 0:
                continue
            guid = "%s-%s" % (set_type, i)
            sentence = line[0]
            label = line[1]
            examples.append(
                InputExample(guid=guid, text_a=sentence, label=label))
        return examples

def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):
    """Loads a data file into a list of `InputBatch`s."""

    label_map = {label : i for i, label in enumerate(label_list)}

    features = []
    for (ex_index, example) in enumerate(examples):
        tokens_a = tokenizer.tokenize(example.text_a)

        tokens_b = None
        if example.text_b:
            tokens_b = tokenizer.tokenize(example.text_b)
            # Modifies `tokens_a` and `tokens_b` in place so that the total
            # length is less than the specified length.
            # Account for [CLS], [SEP], [SEP] with "- 3"
            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)
        else:
            # Account for [CLS] and [SEP] with "- 2"
            if len(tokens_a) > max_seq_length - 2:
                tokens_a = tokens_a[:(max_seq_length - 2)]

        # The convention in BERT is:
        # (a) For sequence pairs:
        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]
        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1
        # (b) For single sequences:
        #  tokens:   [CLS] the dog is hairy . [SEP]
        #  type_ids: 0   0   0   0  0     0 0
        #
        # Where "type_ids" are used to indicate whether this is the first
        # sequence or the second sequence. The embedding vectors for `type=0` and
        # `type=1` were learned during pre-training and are added to the wordpiece
        # embedding vector (and position vector). This is not *strictly* necessary
        # since the [SEP] token unambigiously separates the sequences, but it makes
        # it easier for the model to learn the concept of sequences.
        #
        # For classification tasks, the first vector (corresponding to [CLS]) is
        # used as as the "sentence vector". Note that this only makes sense because
        # the entire model is fine-tuned.
        tokens = ["[CLS]"] + tokens_a + ["[SEP]"]
        segment_ids = [0] * len(tokens)

        if tokens_b:
            tokens += tokens_b + ["[SEP]"]
            segment_ids += [1] * (len(tokens_b) + 1)

        input_ids = tokenizer.convert_tokens_to_ids(tokens)

        # The mask has 1 for real tokens and 0 for padding tokens. Only real
        # tokens are attended to.
        input_mask = [1] * len(input_ids)

        # Zero-pad up to the sequence length.
        padding = [0] * (max_seq_length - len(input_ids))
        input_ids += padding
        input_mask += padding
        segment_ids += padding

        assert len(input_ids) == max_seq_length
        assert len(input_mask) == max_seq_length
        assert len(segment_ids) == max_seq_length

        label_id = label_map[example.label]
        if ex_index < 5:
            logger.info("*** Example ***")
            logger.info("guid: %s" % (example.guid))
            logger.info("tokens: %s" % " ".join(
                    [str(x) for x in tokens]))
            logger.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
            logger.info("input_mask: %s" % " ".join([str(x) for x in input_mask]))
            logger.info(
                    "segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
            logger.info("label: %s (id = %d)" % (example.label, label_id))

        features.append(
                InputFeatures(input_ids=input_ids,
                              input_mask=input_mask,
                              segment_ids=segment_ids,
                              label_id=label_id))
    return features

def _truncate_seq_pair(tokens_a, tokens_b, max_length):
    """Truncates a sequence pair in place to the maximum length."""

    # This is a simple heuristic which will always truncate the longer sequence
    # one token at a time. This makes more sense than truncating an equal percent
    # of tokens from each, since if one sequence is very short then each token
    # that's truncated likely contains more information than a longer sequence.
    while True:
        total_length = len(tokens_a) + len(tokens_b)
        if total_length <= max_length:
            break
        if len(tokens_a) > len(tokens_b):
            tokens_a.pop()
        else:
            tokens_b.pop()

def accuracy(out, labels):
    outputs = np.argmax(out, axis=1)
    return np.sum(outputs == labels)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
data_dir = "./model/wiki_70k_frames"
processor = FramesProcessor()
train_examples = processor.get_train_examples(data_dir)
test_examples = processor.get_test_examples(data_dir)

label_list = processor.get_labels(data_dir=data_dir)

train_features = convert_examples_to_features(train_examples, label_list, 128, tokenizer)
test_features = convert_examples_to_features(test_examples,label_list, 128, tokenizer)

train_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)
train_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)
train_input_masks = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)

test_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)
test_label_ids = torch.tensor([f.label_id for f in test_features], dtype=torch.long)
test_input_masks = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)


# Pre-processing
def preprocess(sentences):
  clean_sentences = []
  for sentence in sentences:
    # Tweet to list of words
    tokens = sentence.split()
    table = str.maketrans('', '', string.punctuation)
    tokens = [w.translate(table).lower() for w in tokens]
    sentence = ' '.join(word for word in tokens)
    clean_sentences.append(sentence)
  #Add begining and end of sentence tokens
  clean_sentences = [sentence + " [SEP] [CLS]" for sentence in clean_sentences]
  return clean_sentences

def bert_model(train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks, epochs = 1, batch_size = 32, lr = 2e-5):
    """
    lr = learning rate
    T = probabilistic threshold
    """

#    tokenized_train = [tokenizer.tokenize(sent) for sent in train_tweets]
#    train_inputs = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_train]
#    for input_ in train_inputs:
#        padding = [0] * (MAX_LEN - len(input_))
#        input_ += padding
#    # Create attention masks
#    train_masks = []
#    # Create a mask of 1s for each token followed by 0s for padding
#    for seq in train_inputs:
#      seq_mask = [float(i>0) for i in seq]
#      train_masks.append(seq_mask)
#
#    tokenized_test = [tokenizer.tokenize(sent) for sent in test_tweets]
#    test_inputs = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_test]
#    for input_ in test_inputs:
#        padding = [0] * (MAX_LEN - len(input_))
#        input_ += padding
#    # Create attention masks
#    test_masks = []
#    # Create a mask of 1s for each token followed by 0s for padding
#    for seq in test_inputs:
#      seq_mask = [float(i>0) for i in seq]
#      test_masks.append(seq_mask)
    # Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, 
    # with an iterator the entire dataset does not need to be loaded into memory

    train_data = TensorDataset(train_inputs, train_masks, train_labels)
    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)

    test_data = TensorDataset(test_inputs, test_masks, test_labels)
    test_sampler = SequentialSampler(test_data)
    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)
    # Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. 
    # model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(label_list))
    config = BertConfig(num_labels=len(label_list))
    if config.vocab_size % 8 != 0:
        config.vocab_size += 8 - (config.vocab_size % 8)
    model_fn = "./model/old/ckpt_0_no_pretraining.pt"
    bert_model = 'bert-base-uncased'    
    model_state_dict = torch.load(model_fn, map_location='cpu')["model"]
    model = BertForSequenceClassification.from_pretrained(bert_model, config = config, state_dict = model_state_dict)
    print(model.num_labels)
    model.cuda()
    param_optimizer = list(model.named_parameters())
    no_decay = ['bias', 'gamma', 'beta']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
        'weight_decay_rate': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
        'weight_decay_rate': 0.0}
    ]
    # This variable contains all of the hyperparemeter information that the training loop needs
    optimizer = AdamW(optimizer_grouped_parameters,
                        lr=lr)
    # Store the loss
    train_loss_set = []
    
    batch_size_ = 0
    # trange is a tqdm wrapper around the normal python range
    for _ in trange(epochs, desc="Epoch"):
      
      # Tracking variables
      tr_loss = 0
      nb_tr_examples, nb_tr_steps = 0, 0
      # Train the data for one epoch
      for step, batch in enumerate(train_dataloader):      
        # Training
        # Set our model to training mode (as opposed to evaluation mode)
        model.train()
        # Add batch to GPU
        batch = tuple(t.to(device) for t in batch)
        # Unpack the inputs from our dataloader
        b_input_ids, b_input_mask, b_labels = batch
        # Clear out the gradients (by default they accumulate)
        optimizer.zero_grad()
        # Forward pass
        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)
        loss = outputs[0]
        logits = outputs[1]
        train_loss_set.append(loss.item())    
        # Backward pass
        loss.backward()
        # Update parameters and take a step using the computed gradient

        optimizer.step()
        
        
        # Update tracking variables
        tr_loss += loss.item()
        nb_tr_examples += b_input_ids.size(0)
        nb_tr_steps += 1
      print("Train loss: {}".format(tr_loss/nb_tr_steps))
    ###########################################################################################################################################################################
    # test
    # Put model in evaluation mode to evaluate loss on the test set
    model.eval()

    # Tracking variables 
    labels,predictions=[],[]
    # Evaluate data for one epoch
    for t_batch in test_dataloader:
      # Add batch to GPU
      t_batch = tuple(t.to(device) for t in t_batch)
      # Unpack the inputs from our dataloader
      t_b_input_ids, t_b_input_mask, t_b_labels = t_batch
      # Telling the model not to compute or store gradients, saving memory and speeding up test
      with torch.no_grad():
        # Forward pass, calculate logit predictions
        output = model(t_b_input_ids, token_type_ids=None, attention_mask=t_b_input_mask)
        logits = output[0]
      
      # Move logits and labels to CPU

      # If probability greater than or equal to threshold T the tweet contains that emotion
      _, pred = torch.max(logits, 1)
      pred = pred.detach().cpu().numpy().astype(int).tolist()
      label_ids = t_b_labels.to('cpu').numpy().tolist()
      labels+=label_ids
      predictions+=pred
    labels=np.array(labels)
    predictions=np.array(predictions)
    recall_micro=recall_score(labels, predictions, average = "micro")
    recall_macro=recall_score(labels, predictions, average = "macro")
    precision_micro=precision_score(labels, predictions, average = "micro")
    precision_macro=precision_score(labels, predictions, average = "macro")
    f1_micro=f1_score(labels, predictions, average = "micro")
    f1_macro=f1_score(labels, predictions, average = "macro")
    model_metrics={("Precision","Micro"):precision_micro,("Precision","Macro"):precision_macro,
                  ("Recall","Micro"):recall_micro,("Recall","Macro"):recall_macro,
                  ("F1 score","Micro"):f1_micro,("F1 score","Macro"):f1_macro}
    batch_size_ += len(batch[0])
    print("--------------------------------------------------------------")
    print(step)
    print(batch_size_)
    print(model_metrics)
    np.set_printoptions(threshold=sys.maxsize)
    result = compute_metrics("frames", predictions, labels, label_list)
    results = {}
    results.update(result)
    print(results)
    with open("./model/wiki_70k_frames/logfile", "w") as writer:
        logger.info("***** Eval results *****")
        for key in sorted(results.keys()):
            logger.info("  %s = %s", key, str(results[key]))
            writer.write("%s = %s\n" % (key, str(results[key])))
    print(labels)
    print(predictions)
    return model_metrics, results


batch_size = 32
epochs = 1
bert_lr = 2e-5
# testing
bert_test_scores = bert_model(train_input_ids, test_input_ids, train_label_ids, test_label_ids, train_input_masks, test_input_masks, epochs, batch_size, bert_lr)
print(bert_test_scores[0])
print(bert_test_scores[1])